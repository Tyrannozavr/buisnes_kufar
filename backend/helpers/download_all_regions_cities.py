#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –≤—Å–µ—Ö —Å–ø–∏—Å–∫–æ–≤ –≥–æ—Ä–æ–¥–æ–≤ —Å superresearch.ru/?id=808
–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏—Ö –≤ —Ñ–∞–π–ª—ã —Ñ–æ—Ä–º–∞—Ç–∞ "–Ω–∞–∑–≤–∞–Ω–∏–µ –æ–±–ª–∞—Å—Ç–∏.txt"
"""

import asyncio
import re
from pathlib import Path
from typing import List, Dict
import httpx
from bs4 import BeautifulSoup


# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –≤—ã—Ö–æ–¥–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
OUTPUT_DIR = Path("data_cities")
OUTPUT_DIR.mkdir(exist_ok=True)


async def fetch_page(url: str, max_retries: int = 3) -> str:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏"""
    for attempt in range(max_retries):
        try:
            async with httpx.AsyncClient(timeout=30.0, follow_redirects=True) as client:
                response = await client.get(url)
                response.raise_for_status()
                return response.text
        except Exception as e:
            print(f"  ‚ö†Ô∏è  –ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{max_retries} –Ω–µ —É–¥–∞–ª–∞—Å—å –¥–ª—è {url}: {e}")
            if attempt < max_retries - 1:
                await asyncio.sleep(2)
            else:
                raise
    return ""


def parse_region_links(html: str) -> Dict[str, str]:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ä–µ–≥–∏–æ–Ω—ã —Å –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã superresearch.ru/?id=808
    
    Returns:
        Dict —Å –∫–ª—é—á–æ–º "–Ω–∞–∑–≤–∞–Ω–∏–µ —Ä–µ–≥–∏–æ–Ω–∞" –∏ –∑–Ω–∞—á–µ–Ω–∏–µ–º "–ø–æ–ª–Ω—ã–π URL"
    """
    soup = BeautifulSoup(html, 'html.parser')
    regions = {}
    
    # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –≤–∏–¥–∞ "–≥–æ—Ä–æ–¥–∞ X –≤ –∞–ª—Ñ–∞–≤–∏—Ç–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ"
    for link in soup.find_all('a', href=True):
        text = link.get_text(strip=True)
        href = link.get('href', '')
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω "–≥–æ—Ä–æ–¥–∞ X –≤ –∞–ª—Ñ–∞–≤–∏—Ç–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ"
        if '–≥–æ—Ä–æ–¥–∞' in text.lower() and '–≤ –∞–ª—Ñ–∞–≤–∏—Ç–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ' in text.lower():
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ —Ä–µ–≥–∏–æ–Ω–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞ —Å—Å—ã–ª–∫–∏
            # –ü—Ä–∏–º–µ—Ä: "–≥–æ—Ä–æ–¥–∞ –†–µ—Å–ø—É–±–ª–∏–∫–∏ –ú–∞—Ä–∏–π –≠–ª –≤ –∞–ª—Ñ–∞–≤–∏—Ç–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ"
            match = re.search(r'–≥–æ—Ä–æ–¥–∞\s+(.+?)\s+–≤\s+–∞–ª—Ñ–∞–≤–∏—Ç–Ω–æ–º\s+–ø–æ—Ä—è–¥–∫–µ', text, re.IGNORECASE)
            if match:
                region_name = match.group(1).strip()
                
                # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π URL
                if href.startswith('?'):
                    full_url = f"https://superresearch.ru{href}"
                elif href.startswith('http'):
                    full_url = href
                else:
                    full_url = f"https://superresearch.ru/{href}"
                
                regions[region_name] = full_url
    
    return regions


def parse_cities_from_html(html: str) -> List[str]:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –≥–æ—Ä–æ–¥–æ–≤ –∏–∑ HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    
    Returns:
        List[str]: –°–ø–∏—Å–æ–∫ –Ω–∞–∑–≤–∞–Ω–∏–π –≥–æ—Ä–æ–¥–æ–≤
    """
    soup = BeautifulSoup(html, 'html.parser')
    
    # –ü–æ–ª—É—á–∞–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    full_text = soup.get_text()
    
    # –ò—â–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω "–ì–æ—Ä–æ–¥–∞ X –≤ –∞–ª—Ñ–∞–≤–∏—Ç–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ:"
    pattern = r'–≥–æ—Ä–æ–¥–∞\s+.*?\s+–≤\s+–∞–ª—Ñ–∞–≤–∏—Ç–Ω–æ–º\s+–ø–æ—Ä—è–¥–∫–µ:'
    match = re.search(pattern, full_text, re.IGNORECASE)
    
    cities = []
    
    if match:
        # –ë–µ—Ä–µ–º —Ç–µ–∫—Å—Ç –ø–æ—Å–ª–µ –º–∞—Ä–∫–µ—Ä–∞
        start_pos = match.end()
        text_after_marker = full_text[start_pos:]
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ —Å—Ç—Ä–æ–∫–∞–º
        lines = text_after_marker.split('\n')
        
        for line in lines:
            line = line.strip()
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
            if not line:
                continue
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –ø–æ—Ö–æ–∂–µ –ª–∏ –Ω–∞ –Ω–∞–∑–≤–∞–Ω–∏–µ –≥–æ—Ä–æ–¥–∞
            # –ò–º—è —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–µ: –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å –∑–∞–≥–ª–∞–≤–Ω–æ–π –±—É–∫–≤—ã
            if len(line) > 2 and len(line) < 60 and line[0].isupper():
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª—É–∂–µ–±–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
                if not any(skip in line for skip in ['¬©', '–¢–µ–ª.', '–†–æ—Å—Å–∏—è', '—É–ª.', '–¥.', '–ø.', '+7', '–≥–ª–∞–≤–Ω–∞—è', 'RAI']):
                    cities.append(line)
                    
                    # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è –∫–æ–≥–¥–∞ –≤—Å—Ç—Ä–µ—á–∞–µ–º –º–∞—Ä–∫–µ—Ä—ã –∫–æ–Ω—Ü–∞ —Å–ø–∏—Å–∫–∞
                    if any(marker in line for marker in ['¬©', '–¢–µ–ª.', 'Created by', 'Powered by']):
                        break
    
    # –û—á–∏—â–∞–µ–º –æ—Ç –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∏ —Ñ–∏–ª—å—Ç—Ä—É–µ–º
    unique_cities = []
    seen = set()
    
    for city in cities:
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º: —É–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ —Å–∏–º–≤–æ–ª—ã
        city_normalized = ' '.join(city.split())
        city_normalized = city_normalized.strip()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –Ω–µ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–æ–µ –∏–ª–∏ –¥–ª–∏–Ω–Ω–æ–µ —Å–ª–æ–≤–æ
        if 2 < len(city_normalized) < 50:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ö–æ—Ç—è –±—ã –æ–¥–Ω—É –±—É–∫–≤—É
            if any(c.isalpha() for c in city_normalized):
                # –î–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É–µ–º lower()
                city_key = city_normalized.lower()
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã (—Ä–µ–≥–∏—Å—Ç—Ä–æ–Ω–µ–∑–∞–≤–∏—Å–∏–º–æ)
                if city_key not in seen:
                    seen.add(city_key)
                    unique_cities.append(city_normalized)
    
    return unique_cities


async def fetch_and_save_region(region_name: str, url: str):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É —Ä–µ–≥–∏–æ–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–ø–∏—Å–æ–∫ –≥–æ—Ä–æ–¥–æ–≤ –≤ —Ñ–∞–π–ª"""
    print(f"  üìç –ó–∞–≥—Ä—É–∂–∞–µ–º: {region_name}")
    
    try:
        html = await fetch_page(url)
        cities = parse_cities_from_html(html)
        
        if cities:
            # –°–æ–∑–¥–∞–µ–º –±–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞
            safe_filename = re.sub(r'[^\w\s-]', '', region_name)
            safe_filename = re.sub(r'\s+', '_', safe_filename)
            filepath = OUTPUT_DIR / f"{safe_filename}.txt"
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≥–æ—Ä–æ–¥–∞ –≤ —Ñ–∞–π–ª
            with open(filepath, 'w', encoding='utf-8') as f:
                for city in cities:
                    f.write(f"{city}\n")
            
            print(f"    ‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(cities)} –≥–æ—Ä–æ–¥–æ–≤ –≤ {filepath}")
        else:
            print(f"    ‚ö†Ô∏è  –ì–æ—Ä–æ–¥–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –¥–ª—è {region_name}")
            
    except Exception as e:
        print(f"    ‚ùå –û—à–∏–±–∫–∞ –¥–ª—è {region_name}: {e}")


async def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É –≤—Å–µ—Ö —Ä–µ–≥–∏–æ–Ω–æ–≤ —Å superresearch.ru...\n")
    
    # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
    main_url = "https://superresearch.ru/?id=808"
    print(f"üì• –ó–∞–≥—Ä—É–∂–∞–µ–º –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É: {main_url}")
    
    try:
        main_html = await fetch_page(main_url)
        print("‚úÖ –ì–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞\n")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {e}")
        return
    
    # 2. –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ä–µ–≥–∏–æ–Ω—ã
    print("üîç –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ä–µ–≥–∏–æ–Ω—ã...")
    regions = parse_region_links(main_html)
    print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ —Ä–µ–≥–∏–æ–Ω–æ–≤: {len(regions)}\n")
    
    if not regions:
        print("‚ùå –†–µ–≥–∏–æ–Ω—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ!")
        return
    
    # 3. –í—ã–≤–æ–¥–∏–º —Å–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ä–µ–≥–∏–æ–Ω–æ–≤
    print("üìã –ù–∞–π–¥–µ–Ω–Ω—ã–µ —Ä–µ–≥–∏–æ–Ω—ã:")
    for i, region_name in enumerate(regions.keys(), 1):
        print(f"  {i}. {region_name}")
    
    print(f"\n{'='*60}")
    print(f"üåç –ù–∞—á–∏–Ω–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É –≥–æ—Ä–æ–¥–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ä–µ–≥–∏–æ–Ω–∞...")
    print(f"{'='*60}\n")
    
    # 4. –ó–∞–≥—Ä—É–∂–∞–µ–º –≥–æ—Ä–æ–¥–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ä–µ–≥–∏–æ–Ω–∞ (–º–∞–∫—Å–∏–º—É–º 2-3 –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤)
    semaphore = asyncio.Semaphore(2)  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 2 –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
    
    async def fetch_with_limit(region_name: str, url: str):
        async with semaphore:
            await fetch_and_save_region(region_name, url)
    
    tasks = [
        fetch_with_limit(region_name, url)
        for region_name, url in regions.items()
    ]
    
    await asyncio.gather(*tasks)
    
    # 5. –ò—Ç–æ–≥–∏
    saved_files = list(OUTPUT_DIR.glob("*.txt"))
    print(f"\n{'='*60}")
    print(f"üéâ –ó–ê–í–ï–†–®–ï–ù–û!")
    print(f"{'='*60}")
    print(f"üìä –í—Å–µ–≥–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(saved_files)}")
    print(f"üìÅ –§–∞–π–ª—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {OUTPUT_DIR.absolute()}")


if __name__ == "__main__":
    asyncio.run(main())

